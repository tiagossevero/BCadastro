# üìù Notas T√©cnicas - Gerador de Data Schemas

## ‚ö†Ô∏è Problema Resolvido: Conflito com `sum()` do PySpark

### üêõ Descri√ß√£o do Problema

Quando voc√™ usa:
```python
from pyspark.sql.functions import *
```

A fun√ß√£o `sum()` **nativa do Python** √© **substitu√≠da** pela fun√ß√£o `pyspark.sql.functions.sum()`, que espera uma **coluna PySpark** como argumento, n√£o n√∫meros ou listas.

### ‚ùå Exemplo do Erro

```python
from pyspark.sql.functions import *

# Isso FALHA com PySparkTypeError
lista1 = [1, 2, 3]
lista2 = [4, 5, 6]
total = sum([len(lista1), len(lista2)])  # ‚ùå ERRO!
# PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.
```

**Por qu√™?** Porque `sum()` agora √© `pyspark.sql.functions.sum()` que espera uma coluna, n√£o n√∫meros.

### ‚úÖ Solu√ß√£o Implementada

#### Op√ß√£o 1: Usar `builtins.sum()` (Recomendado)

```python
from pyspark.sql.functions import *
import builtins  # Importar m√≥dulo builtins

# Funciona! Usa a fun√ß√£o sum() nativa do Python
lista1 = [1, 2, 3]
lista2 = [4, 5, 6]
total = builtins.sum([len(lista1), len(lista2)])  # ‚úÖ OK!
```

#### Op√ß√£o 2: Calcular explicitamente

```python
from pyspark.sql.functions import *

# Funciona! Soma expl√≠cita
lista1 = [1, 2, 3]
lista2 = [4, 5, 6]
total = len(lista1) + len(lista2)  # ‚úÖ OK!
```

#### Op√ß√£o 3: Importar seletivamente (Alternativa)

```python
# Importar apenas as fun√ß√µes que voc√™ precisa
from pyspark.sql.functions import col, count, avg  # etc...

# sum() continua sendo a fun√ß√£o nativa
lista1 = [1, 2, 3]
lista2 = [4, 5, 6]
total = sum([len(lista1), len(lista2)])  # ‚úÖ OK!
```

---

## üìö Implementa√ß√£o nos Notebooks

### `gerar_data_schemas.ipynb`

```python
# ‚úÖ C√©lula de Imports
import os
import json
import builtins  # IMPORTANTE: Para usar sum() nativo do Python
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *  # Isso sobrescreve sum()

# ‚úÖ Uso correto ao longo do notebook
total_tabelas = builtins.sum([len(TABELAS_ORIGINAIS), len(TABELAS_INTERMEDIARIAS)])
```

### `gerar_schemas_simples.ipynb`

```python
# ‚úÖ C√©lula de Configura√ß√£o
import os
import builtins  # IMPORTANTE: Para evitar conflito com pyspark.sql.functions.sum()
from datetime import datetime
from pyspark.sql.types import *
from pyspark.sql.functions import *

# ‚úÖ Uso correto
total = builtins.sum([len(TABELAS['originais']), len(TABELAS['intermediarias'])])
```

---

## üîç Onde Usar `builtins.sum()`

### ‚úÖ Use `builtins.sum()` quando:

1. **Somar n√∫meros inteiros:**
   ```python
   total = builtins.sum([1, 2, 3, 4, 5])
   ```

2. **Somar comprimentos de listas:**
   ```python
   total_itens = builtins.sum([len(lista1), len(lista2), len(lista3)])
   ```

3. **Qualquer opera√ß√£o com n√∫meros nativos do Python:**
   ```python
   total_arquivos = builtins.sum([sucesso, falha])
   ```

### ‚úÖ Use `sum()` do PySpark quando:

1. **Somar valores de uma coluna DataFrame:**
   ```python
   from pyspark.sql.functions import sum

   df.select(sum("valor_coluna")).show()
   ```

2. **Agrega√ß√µes em DataFrames:**
   ```python
   df.groupBy("categoria").agg(sum("quantidade"))
   ```

---

## üìä Tabela Comparativa

| Opera√ß√£o | Fun√ß√£o Correta | Exemplo |
|----------|---------------|---------|
| Somar lista de n√∫meros | `builtins.sum()` | `builtins.sum([1, 2, 3])` |
| Somar comprimentos | `builtins.sum()` | `builtins.sum([len(a), len(b)])` |
| Somar coluna DataFrame | `pyspark.sql.functions.sum()` | `df.select(sum("coluna"))` |
| Agrega√ß√£o DataFrame | `pyspark.sql.functions.sum()` | `df.agg(sum("valor"))` |

---

## üéØ Boas Pr√°ticas

### ‚úÖ Recomendado

```python
import builtins
from pyspark.sql.functions import *

# Para n√∫meros/listas
resultado = builtins.sum([10, 20, 30])

# Para DataFrames
df.select(sum("coluna")).show()
```

### ‚ö†Ô∏è Alternativa (mais verbosa)

```python
from pyspark.sql.functions import col, count, avg, sum as spark_sum

# Para n√∫meros/listas
resultado = sum([10, 20, 30])  # sum() nativo

# Para DataFrames
df.select(spark_sum("coluna")).show()  # spark_sum()
```

### ‚ùå Evitar

```python
from pyspark.sql.functions import *

# ERRO! sum() agora √© do PySpark
resultado = sum([10, 20, 30])  # ‚ùå PySparkTypeError
```

---

## üîß Outros Conflitos Comuns

Al√©m de `sum()`, outras fun√ß√µes do Python podem ser sobrescritas:

| Fun√ß√£o Python | Fun√ß√£o PySpark | Solu√ß√£o |
|--------------|----------------|---------|
| `sum()` | `pyspark.sql.functions.sum()` | `builtins.sum()` |
| `min()` | `pyspark.sql.functions.min()` | `builtins.min()` |
| `max()` | `pyspark.sql.functions.max()` | `builtins.max()` |
| `abs()` | `pyspark.sql.functions.abs()` | `builtins.abs()` |
| `round()` | `pyspark.sql.functions.round()` | `builtins.round()` |

### Exemplo Completo

```python
import builtins
from pyspark.sql.functions import *

# ‚úÖ Fun√ß√µes nativas do Python
lista = [10, -5, 20, -15]
total = builtins.sum(lista)           # Soma
minimo = builtins.min(lista)          # M√≠nimo
maximo = builtins.max(lista)          # M√°ximo
absoluto = builtins.abs(-10)          # Valor absoluto
arredondado = builtins.round(3.7)     # Arredondamento

# ‚úÖ Fun√ß√µes PySpark em DataFrames
df.select(
    sum("valor"),
    min("valor"),
    max("valor"),
    abs(col("valor")),
    round(col("valor"), 2)
).show()
```

---

## üìö Refer√™ncias

### Documenta√ß√£o Oficial

- [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)
- [Python Built-in Functions](https://docs.python.org/3/library/functions.html)
- [Python builtins Module](https://docs.python.org/3/library/builtins.html)

### Mensagens de Erro Comuns

```
PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got int.
```

**Causa:** Tentou usar `sum()` do PySpark com n√∫meros/listas ao inv√©s de colunas.

**Solu√ß√£o:** Use `builtins.sum()` para n√∫meros/listas.

---

## üí° Dica Final

**Sempre importe `builtins` quando usar `from pyspark.sql.functions import *`:**

```python
import builtins  # ‚Üê SEMPRE adicione isso!
from pyspark.sql.functions import *
```

Isso garante que voc√™ sempre ter√° acesso √†s fun√ß√µes nativas do Python sem conflitos.

---

**Criado em:** 2025-11-17
**Vers√£o:** 1.0
**Aplica√ß√£o:** Gerador de Data Schemas BCadastro
